{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQqzAld50o1P"
      },
      "source": [
        "### Part of EECS 6893 - Big Data Analytics - Fall 2021 - Final Project \n",
        "\n",
        "* Group ID:- 202112-53\n",
        "* Title:- Assaying MSD\n",
        "* Contributors:- Karpagam Murugappan; Arya Kasulla\n",
        "\n",
        "Year Prediction using Linear Regression and Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfwyHRgk0t_g",
        "outputId": "8be779bf-443b-4342-a029-0c62b16b1369"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 41.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=00d5295ef1995a4cb50f0d4818bd87058a983020215146874ab18f888ae2b459\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FSLj1duy0o1T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark import SparkConf                                                                                                                 \n",
        "from pyspark.context import SparkContext                                                                                                      \n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['font.size']= 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mi2yiV50o1V"
      },
      "outputs": [],
      "source": [
        "#Create year dict\n",
        "fileObj = open(os.getcwd()+\"/data/YearPredictionMSD/YearPredictionMSD.txt\", \"r\")\n",
        "years = set()\n",
        "for line in fileObj.readlines():\n",
        "    label = int(line[:4])\n",
        "    years.add(label)\n",
        "yr_dict={}\n",
        "ct=0\n",
        "for y in years:\n",
        "    yr_dict[y] = ct\n",
        "    ct+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy57zT8S48FH"
      },
      "outputs": [],
      "source": [
        "fileObj = open(os.getcwd()+\"/data/YearPredictionMSD/YearPredictionMSD.txt\", \"r\")\n",
        "lines = []\n",
        "\n",
        "ct = 0\n",
        "for line in fileObj.readlines():\n",
        "    \n",
        "    label = int(line[:4])\n",
        "    idx = yr_dict[label]\n",
        "    txt=''\n",
        "    txt+=str(idx)\n",
        "    txt+=' '\n",
        "    cc = 0\n",
        "    for v in line.split(',')[1:13]:\n",
        "        cc+=1\n",
        "        txt+=str(cc)\n",
        "        txt+=':'\n",
        "        txt+=v\n",
        "        txt+=' '\n",
        "      \n",
        "      \n",
        "    lines.append(txt)\n",
        "    ct+=1\n",
        "fileObj.close()\n",
        "\n",
        "with open(os.getcwd()+'/data/reg_data_12_features.txt', 'w') as f:\n",
        "    for line in lines:\n",
        "        f.write(line)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4tvrzwZt3ZMv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create spark context and session\n",
        "'''\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAaxF4dh0o1X"
      },
      "source": [
        "#### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RMPy5jdJ0o1X"
      },
      "outputs": [],
      "source": [
        "#dataset = spark.read.csv(os.getcwd()+\"/data/YearPredictionMSD.txt\", header=False, inferSchema=True)\n",
        "dataset = spark.read.csv(\"/content/YearPredictionMSD.txt\", header=False, inferSchema=True)\n",
        "\n",
        "# Using MinMax Scaler to scale 12 features - first column is label - year\n",
        "vassembler = VectorAssembler(inputCols=dataset.columns[1:], outputCol=\"features\")\n",
        "data1 = vassembler.transform(dataset)\n",
        "scaler = MinMaxScaler(inputCol='features', outputCol='feat_scaled')\n",
        "scalermodel = scaler.fit(data1)\n",
        "data2 = scalermodel.transform(data1)\n",
        "data3 = data2.select(F.col('_c0').alias('label'), F.col('feat_scaled').alias('features'))\n",
        "\n",
        "'''\n",
        "80% Training : 20% Testing split; random seed 227\n",
        "'''\n",
        "train, test = data3.randomSplit([0.8,0.2], seed = 227)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQFeY_XR0o1Y",
        "outputId": "2d5b5e06-8d21-42ba-cfe8-9d81818c8164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE:-  9.555613954299925\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "regularization parameter 0.3\n",
        "50 iterations\n",
        "'''\n",
        "lr= LinearRegression(featuresCol = 'features', labelCol='label', maxIter=50, regParam=0.3,elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lr_model = lr.fit(train)\n",
        "training_summary = lr_model.summary\n",
        "\n",
        "# RMSE error\n",
        "print(\"RMSE:- \", training_summary.rootMeanSquaredError)\n",
        "\n",
        "#Get training error evolution\n",
        "objhis = training_summary.objectiveHistory\n",
        "training_error_df = pd.DataFrame(columns=['step', 'training_error'])\n",
        "ct = 0\n",
        "\n",
        "for obj in objhis:\n",
        "    ct += 1\n",
        "    training_error_df = training_error_df.append({'step': ct, 'training_error': 10 ** (obj)}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "NeMUZKOB0o1Y",
        "outputId": "259dc928-89a8-4f67-9079-514b7b45a646"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(8,8))   \n",
        "plt.plot(training_error_df['step'], training_error_df['training_error'],color='blue')\n",
        "plt.ylim(0,5)\n",
        "plt.xlabel('ITERATION')\n",
        "plt.ylabel('TRAINING ERROR')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhruqESS0o1Z",
        "outputId": "ffb48aa4-85d8-4ed1-96aa-65435253b97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing RMSE:-  9.739253241501773\n"
          ]
        }
      ],
      "source": [
        "predictions = lr_model.transform(test)\n",
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
        "print(\"Testing RMSE:- \", evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br9MyGps0o1a"
      },
      "source": [
        "#### Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl5gVawl6Bw-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Read the input in LibSVM format\n",
        "'''\n",
        "dataset = spark.read.format(\"libsvm\").load(os.getcwd()+\"/data/reg_data_12_features.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1S5iyEO6B8N"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "80% Training : 20% Testing split; random seed 100\n",
        "'''\n",
        "train, test = dataset.randomSplit([0.8, 0.2], seed = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpam8GlN3dDK",
        "outputId": "7e32d0cb-732e-47de-d381-4d754f098118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients: \n",
            "DenseMatrix([[-1.76551462e-06,  1.46313750e-07,  2.06855491e-07, ...,\n",
            "              -4.76183213e-06, -3.29518134e-06, -4.24193217e-06],\n",
            "             [-6.00804561e-06, -1.58391972e-06,  2.73548716e-06, ...,\n",
            "               7.20958180e-06,  9.29684987e-06,  4.78056183e-06],\n",
            "             [-1.15817539e-05, -2.33182281e-06,  4.01197356e-06, ...,\n",
            "               1.09816484e-05,  2.04075100e-05,  5.55120074e-06],\n",
            "             ...,\n",
            "             [ 8.00325535e-03, -1.44122644e-04, -4.87278617e-04, ...,\n",
            "               2.60387318e-04,  1.29210340e-03, -1.19602295e-03],\n",
            "             [ 2.64324422e-03, -8.62964018e-05, -1.89851189e-04, ...,\n",
            "              -1.61305445e-04,  1.36416231e-03, -5.68520437e-04],\n",
            "             [ 1.72701887e-06,  2.29068252e-08,  1.14784247e-07, ...,\n",
            "              -2.23426301e-08,  7.82058587e-07,  3.76270156e-07]])\n",
            "Intercept: [-4.50047472743144,-4.682734965156961,-4.345986435185328,-3.5826887633442444,-2.7920140345539997,-2.4804846285160234,-1.967510027827266,-2.7932062752773144,-2.855934218391657,-4.094537270140523,-4.500114960415938,-3.071041309651202,-3.1548590964241607,-3.071396241570727,-3.154652449641175,-3.652339345601643,-2.9576076971712295,-2.6260253231931654,-2.9936572824161005,-3.2458366176045206,-3.583215051034136,-3.5831217703683635,-3.0716710130925855,-3.2953962668090027,-2.397304118336572,-2.652887008317323,-2.4831192326610796,-1.9974665330537575,-2.1301750728609057,-2.0125502068900807,-1.5871907979858004,-1.6719583198524286,-0.8664521953787044,-0.16530216076037874,-0.0981162481004843,-0.12375491029595567,-0.09698162798644262,-0.4512447120534567,-0.185569064592823,-0.10832322064266506,0.3094222282736225,0.36080036759900114,0.5394241295749568,0.7581891924692835,0.9672204868482341,1.0414668785466332,1.2429904553420337,1.2860048488461995,1.179036833937164,1.2859848180323765,1.3969465443447846,1.2241851142976687,1.3432715965893054,1.2084871071229,1.3411588330104702,1.4968239347919736,1.5678543785949712,1.553770156575135,1.572074199470175,1.702122618360626,1.6408217406594459,1.6428645207242345,1.6916319089430871,1.8696439443234267,2.082498523627908,2.203200338640759,2.3860863530946994,2.4856824172854535,2.701116375663368,2.7795261015839596,2.884014957624941,3.028544240819761,3.098541067223641,3.134187497190995,3.1635994790946245,3.2037111499726443,3.290761210411351,3.3449100337739597,3.4254907869509497,3.445194445770502,3.5590832612689836,3.6145580946607585,3.6853714319361552,3.7064816726735965,3.6895107216071006,3.575096280168144,3.46678430323922,2.508077442890309,-5.59932382386393]\n",
            "False positive rate by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.0\n",
            "label 12: 0.0\n",
            "label 13: 0.0\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "label 18: 0.0\n",
            "label 19: 0.0\n",
            "label 20: 0.0\n",
            "label 21: 0.0\n",
            "label 22: 0.0\n",
            "label 23: 0.0\n",
            "label 24: 0.0\n",
            "label 25: 0.0\n",
            "label 26: 0.0\n",
            "label 27: 0.0\n",
            "label 28: 0.0\n",
            "label 29: 0.0\n",
            "label 30: 0.0\n",
            "label 31: 0.0\n",
            "label 32: 0.0\n",
            "label 33: 0.0\n",
            "label 34: 0.0\n",
            "label 35: 0.0\n",
            "label 36: 0.0\n",
            "label 37: 0.0\n",
            "label 38: 0.0\n",
            "label 39: 0.0\n",
            "label 40: 0.0\n",
            "label 41: 0.0\n",
            "label 42: 0.0\n",
            "label 43: 0.0\n",
            "label 44: 0.0\n",
            "label 45: 0.0\n",
            "label 46: 0.0\n",
            "label 47: 0.0\n",
            "label 48: 0.0\n",
            "label 49: 0.0\n",
            "label 50: 0.0\n",
            "label 51: 0.0\n",
            "label 52: 0.0\n",
            "label 53: 0.0\n",
            "label 54: 0.0\n",
            "label 55: 0.0\n",
            "label 56: 0.0\n",
            "label 57: 0.0\n",
            "label 58: 0.0\n",
            "label 59: 0.0\n",
            "label 60: 0.0\n",
            "label 61: 0.0\n",
            "label 62: 0.0\n",
            "label 63: 0.0\n",
            "label 64: 0.0\n",
            "label 65: 0.0\n",
            "label 66: 0.0\n",
            "label 67: 0.0\n",
            "label 68: 0.0\n",
            "label 69: 0.0\n",
            "label 70: 0.0\n",
            "label 71: 0.0\n",
            "label 72: 0.0\n",
            "label 73: 0.0\n",
            "label 74: 0.0\n",
            "label 75: 0.0\n",
            "label 76: 0.0\n",
            "label 77: 0.0\n",
            "label 78: 0.0\n",
            "label 79: 0.0\n",
            "label 80: 0.0\n",
            "label 81: 0.0\n",
            "label 82: 0.003766884690174384\n",
            "label 83: 0.006646576216077762\n",
            "label 84: 0.9892749431068018\n",
            "label 85: 0.0\n",
            "label 86: 0.0\n",
            "label 87: 0.0\n",
            "label 88: 0.0\n",
            "True positive rate by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.0\n",
            "label 12: 0.0\n",
            "label 13: 0.0\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "label 18: 0.0\n",
            "label 19: 0.0\n",
            "label 20: 0.0\n",
            "label 21: 0.0\n",
            "label 22: 0.0\n",
            "label 23: 0.0\n",
            "label 24: 0.0\n",
            "label 25: 0.0\n",
            "label 26: 0.0\n",
            "label 27: 0.0\n",
            "label 28: 0.0\n",
            "label 29: 0.0\n",
            "label 30: 0.0\n",
            "label 31: 0.0\n",
            "label 32: 0.0\n",
            "label 33: 0.0\n",
            "label 34: 0.0\n",
            "label 35: 0.0\n",
            "label 36: 0.0\n",
            "label 37: 0.0\n",
            "label 38: 0.0\n",
            "label 39: 0.0\n",
            "label 40: 0.0\n",
            "label 41: 0.0\n",
            "label 42: 0.0\n",
            "label 43: 0.0\n",
            "label 44: 0.0\n",
            "label 45: 0.0\n",
            "label 46: 0.0\n",
            "label 47: 0.0\n",
            "label 48: 0.0\n",
            "label 49: 0.0\n",
            "label 50: 0.0\n",
            "label 51: 0.0\n",
            "label 52: 0.0\n",
            "label 53: 0.0\n",
            "label 54: 0.0\n",
            "label 55: 0.0\n",
            "label 56: 0.0\n",
            "label 57: 0.0\n",
            "label 58: 0.0\n",
            "label 59: 0.0\n",
            "label 60: 0.0\n",
            "label 61: 0.0\n",
            "label 62: 0.0\n",
            "label 63: 0.0\n",
            "label 64: 0.0\n",
            "label 65: 0.0\n",
            "label 66: 0.0\n",
            "label 67: 0.0\n",
            "label 68: 0.0\n",
            "label 69: 0.0\n",
            "label 70: 0.0\n",
            "label 71: 0.0\n",
            "label 72: 0.0\n",
            "label 73: 0.0\n",
            "label 74: 0.0\n",
            "label 75: 0.0\n",
            "label 76: 0.0\n",
            "label 77: 0.0\n",
            "label 78: 0.0\n",
            "label 79: 0.0\n",
            "label 80: 0.0\n",
            "label 81: 0.0\n",
            "label 82: 0.004473712465552414\n",
            "label 83: 0.004999000199960008\n",
            "label 84: 0.9942463328275164\n",
            "label 85: 0.0\n",
            "label 86: 4.0343728567394196e-05\n",
            "label 87: 0.0\n",
            "label 88: 0.0\n",
            "Precision by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.0\n",
            "label 12: 0.0\n",
            "label 13: 0.0\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "label 18: 0.0\n",
            "label 19: 0.0\n",
            "label 20: 0.0\n",
            "label 21: 0.0\n",
            "label 22: 0.0\n",
            "label 23: 0.0\n",
            "label 24: 0.0\n",
            "label 25: 0.0\n",
            "label 26: 0.0\n",
            "label 27: 0.0\n",
            "label 28: 0.0\n",
            "label 29: 0.0\n",
            "label 30: 0.0\n",
            "label 31: 0.0\n",
            "label 32: 0.0\n",
            "label 33: 0.0\n",
            "label 34: 0.0\n",
            "label 35: 0.0\n",
            "label 36: 0.0\n",
            "label 37: 0.0\n",
            "label 38: 0.0\n",
            "label 39: 0.0\n",
            "label 40: 0.0\n",
            "label 41: 0.0\n",
            "label 42: 0.0\n",
            "label 43: 0.0\n",
            "label 44: 0.0\n",
            "label 45: 0.0\n",
            "label 46: 0.0\n",
            "label 47: 0.0\n",
            "label 48: 0.0\n",
            "label 49: 0.0\n",
            "label 50: 0.0\n",
            "label 51: 0.0\n",
            "label 52: 0.0\n",
            "label 53: 0.0\n",
            "label 54: 0.0\n",
            "label 55: 0.0\n",
            "label 56: 0.0\n",
            "label 57: 0.0\n",
            "label 58: 0.0\n",
            "label 59: 0.0\n",
            "label 60: 0.0\n",
            "label 61: 0.0\n",
            "label 62: 0.0\n",
            "label 63: 0.0\n",
            "label 64: 0.0\n",
            "label 65: 0.0\n",
            "label 66: 0.0\n",
            "label 67: 0.0\n",
            "label 68: 0.0\n",
            "label 69: 0.0\n",
            "label 70: 0.0\n",
            "label 71: 0.0\n",
            "label 72: 0.0\n",
            "label 73: 0.0\n",
            "label 74: 0.0\n",
            "label 75: 0.0\n",
            "label 76: 0.0\n",
            "label 77: 0.0\n",
            "label 78: 0.0\n",
            "label 79: 0.0\n",
            "label 80: 0.0\n",
            "label 81: 0.0\n",
            "label 82: 0.07941550190597205\n",
            "label 83: 0.055699962866691426\n",
            "label 84: 0.07701896209295662\n",
            "label 85: 0.0\n",
            "label 86: 1.0\n",
            "label 87: 0.0\n",
            "label 88: 0.0\n",
            "Recall by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.0\n",
            "label 12: 0.0\n",
            "label 13: 0.0\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "label 18: 0.0\n",
            "label 19: 0.0\n",
            "label 20: 0.0\n",
            "label 21: 0.0\n",
            "label 22: 0.0\n",
            "label 23: 0.0\n",
            "label 24: 0.0\n",
            "label 25: 0.0\n",
            "label 26: 0.0\n",
            "label 27: 0.0\n",
            "label 28: 0.0\n",
            "label 29: 0.0\n",
            "label 30: 0.0\n",
            "label 31: 0.0\n",
            "label 32: 0.0\n",
            "label 33: 0.0\n",
            "label 34: 0.0\n",
            "label 35: 0.0\n",
            "label 36: 0.0\n",
            "label 37: 0.0\n",
            "label 38: 0.0\n",
            "label 39: 0.0\n",
            "label 40: 0.0\n",
            "label 41: 0.0\n",
            "label 42: 0.0\n",
            "label 43: 0.0\n",
            "label 44: 0.0\n",
            "label 45: 0.0\n",
            "label 46: 0.0\n",
            "label 47: 0.0\n",
            "label 48: 0.0\n",
            "label 49: 0.0\n",
            "label 50: 0.0\n",
            "label 51: 0.0\n",
            "label 52: 0.0\n",
            "label 53: 0.0\n",
            "label 54: 0.0\n",
            "label 55: 0.0\n",
            "label 56: 0.0\n",
            "label 57: 0.0\n",
            "label 58: 0.0\n",
            "label 59: 0.0\n",
            "label 60: 0.0\n",
            "label 61: 0.0\n",
            "label 62: 0.0\n",
            "label 63: 0.0\n",
            "label 64: 0.0\n",
            "label 65: 0.0\n",
            "label 66: 0.0\n",
            "label 67: 0.0\n",
            "label 68: 0.0\n",
            "label 69: 0.0\n",
            "label 70: 0.0\n",
            "label 71: 0.0\n",
            "label 72: 0.0\n",
            "label 73: 0.0\n",
            "label 74: 0.0\n",
            "label 75: 0.0\n",
            "label 76: 0.0\n",
            "label 77: 0.0\n",
            "label 78: 0.0\n",
            "label 79: 0.0\n",
            "label 80: 0.0\n",
            "label 81: 0.0\n",
            "label 82: 0.004473712465552414\n",
            "label 83: 0.004999000199960008\n",
            "label 84: 0.9942463328275164\n",
            "label 85: 0.0\n",
            "label 86: 4.0343728567394196e-05\n",
            "label 87: 0.0\n",
            "label 88: 0.0\n",
            "F-measure by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.0\n",
            "label 12: 0.0\n",
            "label 13: 0.0\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "label 18: 0.0\n",
            "label 19: 0.0\n",
            "label 20: 0.0\n",
            "label 21: 0.0\n",
            "label 22: 0.0\n",
            "label 23: 0.0\n",
            "label 24: 0.0\n",
            "label 25: 0.0\n",
            "label 26: 0.0\n",
            "label 27: 0.0\n",
            "label 28: 0.0\n",
            "label 29: 0.0\n",
            "label 30: 0.0\n",
            "label 31: 0.0\n",
            "label 32: 0.0\n",
            "label 33: 0.0\n",
            "label 34: 0.0\n",
            "label 35: 0.0\n",
            "label 36: 0.0\n",
            "label 37: 0.0\n",
            "label 38: 0.0\n",
            "label 39: 0.0\n",
            "label 40: 0.0\n",
            "label 41: 0.0\n",
            "label 42: 0.0\n",
            "label 43: 0.0\n",
            "label 44: 0.0\n",
            "label 45: 0.0\n",
            "label 46: 0.0\n",
            "label 47: 0.0\n",
            "label 48: 0.0\n",
            "label 49: 0.0\n",
            "label 50: 0.0\n",
            "label 51: 0.0\n",
            "label 52: 0.0\n",
            "label 53: 0.0\n",
            "label 54: 0.0\n",
            "label 55: 0.0\n",
            "label 56: 0.0\n",
            "label 57: 0.0\n",
            "label 58: 0.0\n",
            "label 59: 0.0\n",
            "label 60: 0.0\n",
            "label 61: 0.0\n",
            "label 62: 0.0\n",
            "label 63: 0.0\n",
            "label 64: 0.0\n",
            "label 65: 0.0\n",
            "label 66: 0.0\n",
            "label 67: 0.0\n",
            "label 68: 0.0\n",
            "label 69: 0.0\n",
            "label 70: 0.0\n",
            "label 71: 0.0\n",
            "label 72: 0.0\n",
            "label 73: 0.0\n",
            "label 74: 0.0\n",
            "label 75: 0.0\n",
            "label 76: 0.0\n",
            "label 77: 0.0\n",
            "label 78: 0.0\n",
            "label 79: 0.0\n",
            "label 80: 0.0\n",
            "label 81: 0.0\n",
            "label 82: 0.008470269354565475\n",
            "label 83: 0.009174592495183338\n",
            "label 84: 0.14296331820361705\n",
            "label 85: 0.0\n",
            "label 86: 8.068420203324188e-05\n",
            "label 87: 0.0\n",
            "label 88: 0.0\n",
            "Accuracy:- 0.07689119723515483\n",
            "FPR:- 0.0765796012482087\n",
            "TPR:- 0.07689119723515485\n",
            "F-measure:- 0.012205686786228605\n",
            "Precision:- 0.07540685111923665\n",
            "Recall:- 0.07689119723515485\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "regularization parameter 0.3\n",
        "10 iterations\n",
        "'''\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3)\n",
        "\n",
        "# Fit the model\n",
        "log_model = lr.fit(train)\n",
        "\n",
        "# Coefficients and intercept for multinomial logistic regression\n",
        "print(\"Coefficients: \\n\" + str(log_model.coefficientMatrix))\n",
        "print(\"Intercept: \" + str(log_model.interceptVector))\n",
        "\n",
        "training_summary = log_model.summary\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"False positive rate by label:\")\n",
        "for i, rate in enumerate(training_summary.falsePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"True positive rate by label:\")\n",
        "for i, rate in enumerate(training_summary.truePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"Precision by label:\")\n",
        "for i, prec in enumerate(training_summary.precisionByLabel):\n",
        "    print(\"label %d: %s\" % (i, prec))\n",
        "\n",
        "print(\"Recall by label:\")\n",
        "for i, rec in enumerate(training_summary.recallByLabel):\n",
        "    print(\"label %d: %s\" % (i, rec))\n",
        "\n",
        "print(\"F-measure by label:\")\n",
        "for i, f in enumerate(training_summary.fMeasureByLabel()):\n",
        "    print(\"label %d: %s\" % (i, f))\n",
        "\n",
        "accuracy = training_summary.accuracy\n",
        "falsePositiveRate = training_summary.weightedFalsePositiveRate\n",
        "truePositiveRate = training_summary.weightedTruePositiveRate\n",
        "fMeasure = training_summary.weightedFMeasure()\n",
        "precision = training_summary.weightedPrecision\n",
        "recall = training_summary.weightedRecall\n",
        "print(\"Accuracy:- %s\\nFPR:- %s\\nTPR:- %s\\nF-measure:- %s\\nPrecision:- %s\\nRecall:- %s\"\n",
        "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3Xq1PwH-Yd7"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "predictions = log_model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5HQfotB_eY8"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uE3vmoC0o1f",
        "outputId": "7d510a1b-a1a1-49e2-e2be-3ddff1019f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Accuracy:- 0.07599088926958418\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing Accuracy:- \" +str(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2UAJQzJ0o1f"
      },
      "source": [
        "#### Reload the dataset and select records in the year 1990 to 2010 i.e. label > 70 - data with high number of records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wViUy7OA9_kJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Read the input in LibSVM format\n",
        "'''\n",
        "dataset = spark.read.format(\"libsvm\").load(os.getcwd()+\"/data/reg_data_12_features.txt\")\n",
        "d1 = dataset.filter(dataset.label > '70')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxi7U-uqBepn",
        "outputId": "bcfffaa8-7fe6-4a6d-fcb3-be020e85aa9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 78.0|(12,[0,1,2,3,4,5,...|\n",
            "| 84.0|(12,[0,1,2,3,4,5,...|\n",
            "+-----+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "d1.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJrFz5uQ-P8H"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "80% Training : 20% Testing split; random seed 235\n",
        "'''\n",
        "train, test = d1.randomSplit([0.8, 0.2], seed = 235)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTfSzGQo-WHr",
        "outputId": "95541dd1-5252-4e5a-9cb2-5e2c0a6543d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients: \n",
            "DenseMatrix([[ 1.77526345e-08,  6.06811064e-10,  4.34770297e-10, ...,\n",
            "              -4.37676958e-09,  4.51627754e-09, -6.80096660e-09],\n",
            "             [ 1.77526345e-08,  6.06811064e-10,  4.34770297e-10, ...,\n",
            "              -4.37676958e-09,  4.51627754e-09, -6.80096660e-09],\n",
            "             [ 1.77526345e-08,  6.06811064e-10,  4.34770297e-10, ...,\n",
            "              -4.37676958e-09,  4.51627754e-09, -6.80096660e-09],\n",
            "             ...,\n",
            "             [ 7.63084433e-03, -2.02646638e-04, -1.56002963e-04, ...,\n",
            "               1.53675141e-05,  1.70004921e-03, -5.11278088e-04],\n",
            "             [ 2.59535829e-03, -1.30561289e-04, -4.82144073e-05, ...,\n",
            "              -5.28937006e-04,  2.20610190e-03, -2.74694832e-04],\n",
            "             [ 2.67111861e-06,  3.96592533e-08,  2.27036256e-07, ...,\n",
            "              -7.57967115e-08,  1.27897382e-06,  7.33863815e-07]])\n",
            "Intercept: [-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,-1.870481220713539,7.63785792833897,7.721740969806383,7.7558772701927,7.734000083149526,7.795710337891192,7.851299535984,7.89515465990583,7.984603159056499,7.97830960999783,8.102259354667853,8.130474316799006,8.19995925556801,8.1933146489079,8.155617901561401,8.015884178227523,7.89777017449016,6.931799542611625,-1.1774662564952225]\n",
            "False positive rate by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 3.399857205997348e-06\n",
            "label 11: 0.009817013626498\n",
            "label 12: 0.030470471305351098\n",
            "label 13: 0.9588550421269294\n",
            "label 14: 3.4494653328734047e-06\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "True positive rate by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.01203050592574027\n",
            "label 12: 0.027620254853559276\n",
            "label 13: 0.9681690051626389\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "Precision by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.10562716127004086\n",
            "label 12: 0.08628595248020009\n",
            "label 13: 0.10023839739238506\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "Recall by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.01203050592574027\n",
            "label 12: 0.027620254853559276\n",
            "label 13: 0.9681690051626389\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "F-measure by label:\n",
            "label 0: 0.0\n",
            "label 1: 0.0\n",
            "label 2: 0.0\n",
            "label 3: 0.0\n",
            "label 4: 0.0\n",
            "label 5: 0.0\n",
            "label 6: 0.0\n",
            "label 7: 0.0\n",
            "label 8: 0.0\n",
            "label 9: 0.0\n",
            "label 10: 0.0\n",
            "label 11: 0.021600771456123432\n",
            "label 12: 0.041845656238944755\n",
            "label 13: 0.18166798404878076\n",
            "label 14: 0.0\n",
            "label 15: 0.0\n",
            "label 16: 0.0\n",
            "label 17: 0.0\n",
            "Accuracy:- 0.09987033097075523\n",
            "FPR:- 0.09901970735207262\n",
            "TPR:- 0.09987033097075523\n",
            "F-measure:- 0.023899198976185313\n",
            "Precision:- 0.027386502434228164\n",
            "Recall:- 0.09987033097075523\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "regularization parameter 0.23\n",
        "10 iterations\n",
        "'''\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.23)\n",
        "\n",
        "# Fit the model\n",
        "log_model = lr.fit(train)\n",
        "\n",
        "# Coefficients and intercept for multinomial logistic regression\n",
        "print(\"Coefficients: \\n\" + str(log_model.coefficientMatrix))\n",
        "print(\"Intercept: \" + str(log_model.interceptVector))\n",
        "\n",
        "training_summary = log_model.summary\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"False positive rate by label:\")\n",
        "for i, rate in enumerate(training_summary.falsePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"True positive rate by label:\")\n",
        "for i, rate in enumerate(training_summary.truePositiveRateByLabel):\n",
        "    print(\"label %d: %s\" % (i, rate))\n",
        "\n",
        "print(\"Precision by label:\")\n",
        "for i, prec in enumerate(training_summary.precisionByLabel):\n",
        "    print(\"label %d: %s\" % (i, prec))\n",
        "\n",
        "print(\"Recall by label:\")\n",
        "for i, rec in enumerate(training_summary.recallByLabel):\n",
        "    print(\"label %d: %s\" % (i, rec))\n",
        "\n",
        "print(\"F-measure by label:\")\n",
        "for i, f in enumerate(training_summary.fMeasureByLabel()):\n",
        "    print(\"label %d: %s\" % (i, f))\n",
        "\n",
        "accuracy = training_summary.accuracy\n",
        "falsePositiveRate = training_summary.weightedFalsePositiveRate\n",
        "truePositiveRate = training_summary.weightedTruePositiveRate\n",
        "fMeasure = training_summary.weightedFMeasure()\n",
        "precision = training_summary.weightedPrecision\n",
        "recall = training_summary.weightedRecall\n",
        "print(\"Accuracy:- %s\\nFPR:- %s\\nTPR:- %s\\nF-measure:- %s\\nPrecision:- %s\\nRecall:- %s\"\n",
        "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hma2giO_uMG"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "predictions = log_model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54JuPIHI_0Ds"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pva5tduW0o1i",
        "outputId": "c6a23078-5e1f-4045-86e8-6e90aebe441b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Accuracy:- 0.09972295680644755\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing Accuracy:- \" +str(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2ahnpcCASGq",
        "outputId": "57e395bc-5c66-46f6-ce41-02c71100c96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704810740507...|[3.18461987510666...|      83.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704808216143...|[3.21406033189982...|      83.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704809171308...|[3.19766757546525...|      83.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704806554973...|[3.26860972304081...|      86.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704804999717...|[3.15981228911582...|      84.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704807052505...|[3.18510026519851...|      84.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704811657942...|[3.24779072004378...|      83.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704806050788...|[3.21054427586435...|      83.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704806126200...|[3.27885465828350...|      84.0|\n",
            "| 86.0|(12,[0,1,2,3,4,5,...|[-1.8704808304478...|[3.19087243083618...|      84.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.filter(predictions.label > '85').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnVBcxpYDjGW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "regression_bda_00.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}